{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ea50c79",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1ed6f67-6f75-460b-a94c-a913175f5d73"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from torchvision import transforms"
      ],
      "metadata": {
        "id": "HPZBd4Ul_RCT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_depth_as_rgb(path):\n",
        "    img = Image.open(path)\n",
        "\n",
        "    if img.mode != \"RGB\":\n",
        "        img = img.convert(\"RGB\")\n",
        "\n",
        "    return img"
      ],
      "metadata": {
        "id": "RCwjwWfs_61T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "metadata": {
        "id": "Tk_BoJlLEido"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DPT (MiDaS) Encoder Embeddings"
      ],
      "metadata": {
        "id": "QNNR9QK6ADYp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.hub\n",
        "\n",
        "midas = torch.hub.load(\"intel-isl/MiDaS\", \"DPT_Large\").to(device)\n",
        "midas.eval()\n",
        "midas_transforms = torch.hub.load(\"intel-isl/MiDaS\", \"transforms\")\n",
        "dpt_transform = midas_transforms.dpt_transform"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ITrW8mzOABXP",
        "outputId": "3843f9ab-a32d-4dc9-e6df-5a3a07cf0bcf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/intel-isl_MiDaS_master\n",
            "Using cache found in /root/.cache/torch/hub/intel-isl_MiDaS_master\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Depth-MAE Encoder Embeddings"
      ],
      "metadata": {
        "id": "RFl49floAKwQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import ViTMAEModel\n",
        "\n",
        "mae_model = ViTMAEModel.from_pretrained(\"facebook/vit-mae-base\").to(device)\n",
        "mae_model.eval()\n",
        "mae_transform = transforms.Compose([\n",
        "    transforms.Resize((224,224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5,0.5,0.5],[0.5,0.5,0.5])\n",
        "])"
      ],
      "metadata": {
        "id": "NuvPjg2iALij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DepthAnythingV2 Encoder Embeddings"
      ],
      "metadata": {
        "id": "xnch3ecHAScb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "da_pipe = pipeline(\n",
        "    task=\"depth-estimation\",\n",
        "    model=\"depth-anything/Depth-Anything-V2-Small-hf\",\n",
        "    device=0 if torch.cuda.is_available() else -1\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zpWylIe2ATCL",
        "outputId": "95f2ba64-9aee-4194-d7e6-291c4cedf90a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extract Embeddings"
      ],
      "metadata": {
        "id": "JxeXzqrzA8Oo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_depth_as_rgb(path):\n",
        "    img = Image.open(path)\n",
        "    if img.mode != \"RGB\":\n",
        "        img = img.convert(\"RGB\")\n",
        "    return img\n",
        "\n",
        "@torch.no_grad()\n",
        "def extract_dpt_embedding(img: Image.Image):\n",
        "    \"\"\"\n",
        "    Extract a fixed-size embedding from DPT-Large MiDaS\n",
        "    \"\"\"\n",
        "    # 1. Convert PIL -> numpy, normalize\n",
        "    img_np = np.array(img).astype(np.float32) / 255.0  # HWC, float32\n",
        "    # HWC -> CHW\n",
        "    img_tensor = torch.from_numpy(img_np).permute(2,0,1).unsqueeze(0).to(device)  # [1,3,H,W]\n",
        "\n",
        "    # 2. Forward through model\n",
        "    out = midas(img_tensor)  # shape [1,H,W] for depth map\n",
        "\n",
        "    # 3. Global pooling to get fixed-size vector\n",
        "    pooled = torch.mean(out, dim=[1,2])  # mean over H,W -> shape [B]\n",
        "\n",
        "    return pooled.squeeze(0).cpu().numpy()\n",
        "\n",
        "@torch.no_grad()\n",
        "def extract_mae_embedding(img: Image.Image):\n",
        "    x = mae_transform(img).unsqueeze(0).to(device)\n",
        "    out = mae_model(pixel_values=x)\n",
        "    cls = out.last_hidden_state[:,0]\n",
        "    return cls.squeeze(0).cpu().numpy()\n",
        "\n",
        "from sklearn.preprocessing import normalize\n",
        "\n",
        "@torch.no_grad()\n",
        "def extract_depthanything_embedding(img: Image.Image, target_size=256):\n",
        "    \"\"\"\n",
        "    Use Hugging Face DepthAnythingV2 pipeline to get a fixed-length depth embedding\n",
        "    \"\"\"\n",
        "    depth_map = da_pipe(img)[\"depth\"]  # may be PIL.Image\n",
        "\n",
        "    # Convert to numpy float32\n",
        "    if isinstance(depth_map, Image.Image):\n",
        "        depth_map = np.array(depth_map).astype(np.float32)\n",
        "\n",
        "    # Resize to fixed size\n",
        "    depth_map = cv2.resize(depth_map, (target_size, target_size))\n",
        "\n",
        "    # Normalize\n",
        "    depth_map = (depth_map - depth_map.mean()) / (depth_map.std() + 1e-8)\n",
        "\n",
        "    # Flatten\n",
        "    depth_emb = depth_map.flatten()  # length = target_size*target_size\n",
        "\n",
        "    # Reduce to 512-dim via simple averaging\n",
        "    depth_emb = depth_emb.reshape(512, -1).mean(axis=1)\n",
        "\n",
        "    # L2 normalize\n",
        "    depth_emb = normalize(depth_emb.reshape(1, -1))[0]\n",
        "\n",
        "    return depth_emb"
      ],
      "metadata": {
        "id": "gQJnfTiSESuX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/NNDL Project/clip_embeddings_sunrgbd_with_labels.csv\")\n",
        "\n",
        "dpt_embs = []\n",
        "mae_embs = []\n",
        "da_embs = []\n",
        "\n",
        "for _, row in tqdm(df.iterrows(), total=len(df)):\n",
        "    depth_img = load_depth_as_rgb(row[\"depthmap\"])\n",
        "    dpt_embs.append(extract_dpt_embedding(depth_img))\n",
        "    mae_embs.append(extract_mae_embedding(depth_img))\n",
        "    da_embs.append(extract_depthanything_embedding(depth_img))\n",
        "\n",
        "\n",
        "df_out = df.copy()\n",
        "\n",
        "df_out[\"dpt_embedding\"] = dpt_embs\n",
        "df_out[\"mae_embedding\"] = mae_embs\n",
        "df_out[\"depthanythingv2_embedding\"] = da_embs\n",
        "df_out.to_csv(\"/content/drive/MyDrive/NNDL Project/sunrgbd_depth_embeddings.csv\", index=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zVnb4PtjAcsZ",
        "outputId": "08376879-7d4c-4b9b-8489-4905030dc06a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 16%|█▌        | 219/1385 [2:13:37<11:56:28, 36.87s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ExYd1lluE4pL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}